{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30887,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import MinMaxScaler,QuantileTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n"
      ],
      "metadata": {
        "id": "AkzO4q_u6dhT",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-19T05:22:12.495912Z",
          "iopub.execute_input": "2025-02-19T05:22:12.496323Z",
          "iopub.status.idle": "2025-02-19T05:22:12.501640Z",
          "shell.execute_reply.started": "2025-02-19T05:22:12.496291Z",
          "shell.execute_reply": "2025-02-19T05:22:12.500687Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class Time2Vec(nn.Module):\n",
        "    \"\"\"\n",
        "    Time2Vec converts scalar time inputs into a vector representation using sine and cosine transformations.\n",
        "    This is useful for capturing periodic patterns in time series data.\n",
        "\n",
        "    Args:\n",
        "        num_frequency (int): Number of frequency components for sine and cosine transformations.\n",
        "        period (int): Period of the sine and cosine functions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_frequency=16, period=256):\n",
        "        super(Time2Vec, self).__init__()\n",
        "        self.num_frequency = num_frequency\n",
        "        self.period = period\n",
        "        self.freqs = torch.linspace(1, num_frequency, num_frequency).float()  # Frequency components\n",
        "\n",
        "    def forward(self, t):\n",
        "        \"\"\"\n",
        "        Forward pass for Time2Vec.\n",
        "\n",
        "        Args:\n",
        "            t (torch.Tensor): Input time values of shape (batch_size,).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Concatenated tensor of shape (batch_size, 1 + 2 * num_frequency) containing\n",
        "                           the original time value and its sine/cosine transformations.\n",
        "        \"\"\"\n",
        "        t = t.reshape(-1, 1)  # Reshape for broadcasting\n",
        "        sin_features = torch.sin(2 * torch.pi * self.freqs.reshape(1, -1) * t / self.period)\n",
        "        cos_features = torch.cos(2 * torch.pi * self.freqs.reshape(1, -1) * t / self.period)\n",
        "        time_features = torch.cat([t, sin_features, cos_features], dim=1)  # Concatenate features\n",
        "        return time_features\n",
        "\n"
      ],
      "metadata": {
        "id": "yEag1xqE68rB",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-19T04:54:17.085275Z",
          "iopub.execute_input": "2025-02-19T04:54:17.085628Z",
          "iopub.status.idle": "2025-02-19T04:54:17.092557Z",
          "shell.execute_reply.started": "2025-02-19T04:54:17.085600Z",
          "shell.execute_reply": "2025-02-19T04:54:17.091530Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_sequences(input_features, close_prices, in_seq_length=100, out_seq_len=5):\n",
        "    \"\"\"\n",
        "    Creates input-output sequences for time series data.\n",
        "\n",
        "    Args:\n",
        "        input_features (pd.DataFrame): Input features for the model.\n",
        "        close_prices (pd.Series or np.ndarray): Target close prices.\n",
        "        in_seq_length (int): Length of the input sequence.\n",
        "        out_seq_len (int): Length of the output sequence.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Input sequences of shape (num_samples, in_seq_length, num_features).\n",
        "        np.ndarray: Output sequences of shape (num_samples, out_seq_len).\n",
        "    \"\"\"\n",
        "    x, y = [], []\n",
        "    for i in range(len(input_features) - in_seq_length - out_seq_len):\n",
        "        # Extract input sequence\n",
        "        x.append(input_features.iloc[i:i+in_seq_length].values)\n",
        "\n",
        "        # Extract output sequence and flatten if necessary\n",
        "        y_seq = close_prices[i+in_seq_length:i+in_seq_length+out_seq_len]\n",
        "        if isinstance(y_seq, np.ndarray) and y_seq.ndim > 1:\n",
        "            y_seq = y_seq.flatten()\n",
        "        y.append(y_seq)\n",
        "\n",
        "    return np.array(x), np.array(y)\n"
      ],
      "metadata": {
        "id": "-crBIfzohEGq",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-19T04:54:17.805895Z",
          "iopub.execute_input": "2025-02-19T04:54:17.806337Z",
          "iopub.status.idle": "2025-02-19T04:54:17.813650Z",
          "shell.execute_reply.started": "2025-02-19T04:54:17.806304Z",
          "shell.execute_reply": "2025-02-19T04:54:17.812552Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_time_features(company):\n",
        "    \"\"\"\n",
        "    Creates time-based features using the Time2Vec layer for a given company's timestamp data.\n",
        "\n",
        "    Args:\n",
        "        company (pd.DataFrame): DataFrame containing the company's data with timestamps as the index.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Time features generated by the Time2Vec layer, of shape (num_samples, 1 + 2 * num_frequency).\n",
        "    \"\"\"\n",
        "    # Extract timestamps from the DataFrame index\n",
        "    timestamps = company.index\n",
        "    timestamps = pd.Series(timestamps)\n",
        "\n",
        "    # Convert timestamps to ordinal values (integers representing days since 0001-01-01)\n",
        "    timestamps = timestamps.apply(lambda x: x.toordinal())\n",
        "\n",
        "    # Convert ordinal timestamps to a PyTorch tensor\n",
        "    date_tensor = torch.tensor(timestamps).float()\n",
        "\n",
        "    # Initialize Time2Vec layer and generate time features\n",
        "    time2vec_layer = Time2Vec()\n",
        "    time_features = time2vec_layer(date_tensor)\n",
        "\n",
        "    return time_features\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Tgw3lpMeVOEW",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-19T04:54:18.765139Z",
          "iopub.execute_input": "2025-02-19T04:54:18.765439Z",
          "iopub.status.idle": "2025-02-19T04:54:18.770364Z",
          "shell.execute_reply.started": "2025-02-19T04:54:18.765417Z",
          "shell.execute_reply": "2025-02-19T04:54:18.769264Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class StockTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    A Transformer-based model for stock price prediction.\n",
        "\n",
        "    Args:\n",
        "        input_dim (int): Dimension of the input features.\n",
        "        d_model (int): Dimension of the model (embedding size).\n",
        "        nhead (int): Number of attention heads in the Transformer.\n",
        "        num_encoder_layers (int): Number of layers in the Transformer encoder.\n",
        "        num_decoder_layers (int): Number of layers in the Transformer decoder.\n",
        "        seq_length (int): Length of the input sequence.\n",
        "        hidden_dim (int, optional): Dimension of the hidden layer. Defaults to 128.\n",
        "        dropout_rate (float, optional): Dropout rate. Defaults to 0.3.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, seq_length, hidden_dim=128, dropout_rate=0.3):\n",
        "        super(StockTransformer, self).__init__()\n",
        "        # Linear layer to project input features to the model dimension\n",
        "        self.fc_in = nn.Linear(input_dim, d_model)\n",
        "        # self.dropout1 = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        # Transformer encoder\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead),\n",
        "            num_layers=num_encoder_layers\n",
        "        )\n",
        "\n",
        "        # Transformer decoder\n",
        "        self.transformer_decoder = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead),\n",
        "            num_layers=num_decoder_layers\n",
        "        )\n",
        "        # self.fc_hidden1 = nn.Linear(d_model, hidden_dim * 2)\n",
        "        # self.fc_hidden2 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        # self.fc_out = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        # Hidden fully connected layer\n",
        "        self.fc_hidden = nn.Linear(d_model, hidden_dim)\n",
        "        # self.fc_out = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        # Output layer to predict the closing price\n",
        "        self.fc_out = nn.Linear(hidden_dim, 1)\n",
        "        # self.relu = nn.ReLU()\n",
        "        # self.dropout2 = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        \"\"\"\n",
        "        Forward pass for the StockTransformer model.\n",
        "\n",
        "        Args:\n",
        "            src (torch.Tensor): Source sequence (input features) of shape (batch_size, seq_len, input_dim).\n",
        "            tgt (torch.Tensor): Target sequence (input features for the decoder) of shape (batch_size, seq_len, input_dim).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Predicted output of shape (batch_size, seq_len).\n",
        "        \"\"\"\n",
        "        # Project input features to the model dimension\n",
        "        src = self.fc_in(src)\n",
        "        tgt = self.fc_in(tgt)\n",
        "        # src = self.dropout1(src)\n",
        "        # tgt = self.dropout1(tgt)\n",
        "\n",
        "        # Permute tensors to (seq_len, batch_size, d_model) for Transformer\n",
        "        src = src.permute(1, 0, 2)\n",
        "        tgt = tgt.permute(1, 0, 2)\n",
        "\n",
        "        # Pass through the Transformer encoder and decoder\n",
        "        memory = self.transformer_encoder(src)\n",
        "        output = self.transformer_decoder(tgt, memory)\n",
        "\n",
        "        # Permute back to (batch_size, seq_len, d_model)\n",
        "        output = output.permute(1, 0, 2)\n",
        "        # output = self.dropout2(output)\n",
        "        # output = self.relu(self.fc_hidden2(output))\n",
        "        # output = self.relu(self.fc_hidden3(output))\n",
        "\n",
        "        # Reshape and pass through the hidden layer\n",
        "        batch_size, seq_len, _ = output.shape\n",
        "        output = output.reshape(-1, output.size(-1))\n",
        "        output = self.fc_hidden(output)\n",
        "        output = self.dropout2(output)\n",
        "\n",
        "        # Final output layer to predict the closing price\n",
        "        output = self.fc_out(output)\n",
        "        # output = self.fc_hidden1(output)\n",
        "        # # output = self.dropout2(output)\n",
        "        # output = self.fc_hidden2(output)\n",
        "        # output = self.fc_out(output)\n",
        "\n",
        "        # Reshape back to (batch_size, seq_len)\n",
        "        output = output.view(batch_size, seq_len)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "0zFGATtWLXet",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-19T04:54:19.452748Z",
          "iopub.execute_input": "2025-02-19T04:54:19.453151Z",
          "iopub.status.idle": "2025-02-19T04:54:19.460932Z",
          "shell.execute_reply.started": "2025-02-19T04:54:19.453122Z",
          "shell.execute_reply": "2025-02-19T04:54:19.460126Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_batch(batch, batch_size):\n",
        "    \"\"\"\n",
        "    Pads a batch of tensors to the specified batch size with zeros if necessary.\n",
        "\n",
        "    Args:\n",
        "        batch (torch.Tensor): The input batch of tensors.\n",
        "        batch_size (int): The desired batch size after padding.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The padded batch with the specified batch size.\n",
        "    \"\"\"\n",
        "    current_size = batch.shape[0]\n",
        "\n",
        "    if current_size < batch_size:\n",
        "        padding = torch.zeros((batch_size - current_size, *batch.shape[1:])).to(batch.device)\n",
        "        padded_batch = torch.cat([batch, padding], dim=0)\n",
        "        return padded_batch\n",
        "\n",
        "    return batch"
      ],
      "metadata": {
        "id": "VG9kJyjqENKj",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-19T04:54:20.293745Z",
          "iopub.execute_input": "2025-02-19T04:54:20.294126Z",
          "iopub.status.idle": "2025-02-19T04:54:20.299049Z",
          "shell.execute_reply.started": "2025-02-19T04:54:20.294095Z",
          "shell.execute_reply": "2025-02-19T04:54:20.298101Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def create_input(stock_name):\n",
        "    \"\"\"\n",
        "    Creates input features for stock data including MACD, EMA, and scaled features.\n",
        "\n",
        "    Args:\n",
        "        stock_data (pd.DataFrame): The input stock data.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the input features, feature scaler, time scaler,\n",
        "               close scaler, and scaled close price.\n",
        "    \"\"\"\n",
        "    # Fetch historical data for the given stock\n",
        "    stock_ticker = yf.Ticker(stock_name)\n",
        "    stock_data = stock_ticker.history(period='max')\n",
        "\n",
        "    # Calculate 12-day and 24-day EMAs\n",
        "    stock_data['12day_EMA'] = stock_data['Close'].ewm(span=12, adjust=False).mean()\n",
        "    stock_data['24day_EMA'] = stock_data['Close'].ewm(span=24, adjust=False).mean()\n",
        "    stock_data['MACD'] = stock_data['12day_EMA'] - stock_data['24day_EMA']\n",
        "\n",
        "    features = stock_data.drop(columns=[\"Date\"])\n",
        "    print(stock_data)\n",
        "\n",
        "    time_features = create_time_features(stock_data)\n",
        "    time_features_df = pd.DataFrame(time_features)\n",
        "    print(time_features_df.shape)\n",
        "\n",
        "    # Extract closing price as target\n",
        "    close_price = features['Close'].values.reshape(-1, 1)\n",
        "    close_price = np.log1p(features['Close'].values.reshape(-1, 1))\n",
        "\n",
        "    # Scale features\n",
        "    feature_scaler = MinMaxScaler()\n",
        "    time_scaler = MinMaxScaler()\n",
        "    close_scaler = QuantileTransformer(output_distribution='normal')\n",
        "    # close_scaler=MinMaxScaler()\n",
        "    # Scale time features - fix dtype issue\n",
        "    time_column = time_features_df.iloc[:, 0].values.reshape(-1, 1)\n",
        "    time_features_transformed = time_scaler.fit_transform(time_column).astype(np.float32)\n",
        "    time_features_df = pd.DataFrame(time_features)\n",
        "    time_features_df.iloc[:, 0] = time_features_transformed.flatten()\n",
        "\n",
        "    # Scale all features\n",
        "    scaled_features = pd.DataFrame(feature_scaler.fit_transform(features))\n",
        "\n",
        "    # Scale close price separately\n",
        "    scaled_close = close_scaler.fit_transform(close_price)\n",
        "\n",
        "    # Concatenate all features\n",
        "    input_features = pd.concat([time_features_df, scaled_features], axis=1)\n",
        "\n",
        "    return input_features, feature_scaler, time_scaler, close_scaler, scaled_close"
      ],
      "metadata": {
        "id": "4ALwokwXR3yh",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-19T05:14:11.790263Z",
          "iopub.execute_input": "2025-02-19T05:14:11.790609Z",
          "iopub.status.idle": "2025-02-19T05:14:11.797849Z",
          "shell.execute_reply.started": "2025-02-19T05:14:11.790584Z",
          "shell.execute_reply": "2025-02-19T05:14:11.796788Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(model, train_loader, test_loader, close_scaler, num_epochs=10, learning_rate=1e-4):\n",
        "    \"\"\"\n",
        "    Trains and evaluates the given model using the provided data loaders, scaler, and hyperparameters.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to be trained and evaluated.\n",
        "        train_loader (torch.utils.data.DataLoader): DataLoader for the training data.\n",
        "        test_loader (torch.utils.data.DataLoader): DataLoader for the testing data.\n",
        "        close_scaler (sklearn.preprocessing.TransformerMixin): Scaler used for the close prices.\n",
        "        num_epochs (int, optional): Number of epochs to train. Defaults to 10.\n",
        "        learning_rate (float, optional): Learning rate for the optimizer. Defaults to 1e-4.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing predictions, targets, test indices, training losses, and validation losses.\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    criterion = nn.HuberLoss(delta=4.0)\n",
        "    # criterion=nn.MSELoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    # For storing predictions\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    test_indices = []\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for batch in train_loader:\n",
        "            src, tgt = batch\n",
        "            src = src.to(device).to(dtype=torch.float32)\n",
        "            tgt = tgt.to(device).to(dtype=torch.float32)\n",
        "            tgt = tgt.unsqueeze(-1)\n",
        "            tgt_input = torch.zeros((tgt.shape[0], tgt.shape[1], src.shape[-1]), device=device)\n",
        "            tgt_input[:, :, 0] = tgt.squeeze(-1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src, tgt_input[:, :-1])\n",
        "            # Use only the last prediction for each sequence\n",
        "            loss = criterion(output[:, -1], tgt.squeeze(-1)[:, -1])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "        train_losses.append(avg_epoch_loss)\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_epoch_loss:.6f}')\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                src, tgt = batch\n",
        "                src = src.to(device).to(dtype=torch.float32)\n",
        "                tgt = tgt.to(device).to(dtype=torch.float32)\n",
        "                tgt = tgt.unsqueeze(-1)\n",
        "                tgt_input = torch.zeros((tgt.shape[0], tgt.shape[1], src.shape[-1]), device=device)\n",
        "                tgt_input[:, :, 0] = tgt.squeeze(-1)\n",
        "\n",
        "                output = model(src, tgt_input[:, :-1])\n",
        "                loss = criterion(output[:, -1], tgt.squeeze(-1)[:, -1])\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(test_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        print(f'Validation Loss: {avg_val_loss:.6f}')\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "    # Load best model for final evaluation\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    model.eval()\n",
        "\n",
        "    # Final evaluation and predictions\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(test_loader):\n",
        "            src, tgt = batch\n",
        "            src = src.to(device).to(dtype=torch.float32)\n",
        "            tgt = tgt.to(device).to(dtype=torch.float32)\n",
        "\n",
        "            tgt = tgt.unsqueeze(-1)\n",
        "            tgt_input = torch.zeros((tgt.shape[0], tgt.shape[1], src.shape[-1]), device=device)\n",
        "            tgt_input[:, :, 0] = tgt.squeeze(-1)\n",
        "\n",
        "            output = model(src, tgt_input[:, :-1])\n",
        "            preds = output[:, -1].cpu().numpy()\n",
        "            targets = tgt[:, -1].cpu().numpy()\n",
        "\n",
        "            all_predictions.extend(preds)\n",
        "            all_targets.extend(targets)\n",
        "            test_indices.extend(list(range(i * test_loader.batch_size,\n",
        "                                          min((i + 1) * test_loader.batch_size, len(test_loader.dataset)))))\n",
        "\n",
        "    # Inverse transform predictions and targets\n",
        "    all_predictions = close_scaler.inverse_transform(\n",
        "        np.array(all_predictions).reshape(-1, 1)).flatten()\n",
        "    all_predictions = np.expm1(\n",
        "        np.array(all_predictions).reshape(-1, 1)).flatten()\n",
        "    all_targets = close_scaler.inverse_transform(\n",
        "        np.array(all_targets).reshape(-1, 1)).flatten()\n",
        "    all_targets = np.expm1(\n",
        "         np.array(all_targets).reshape(-1, 1)).flatten()\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse = np.mean((all_predictions - all_targets)**2)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mape = np.mean(np.abs((all_targets - all_predictions) / all_targets)) * 100\n",
        "\n",
        "    print(f'Test MSE: {mse:.4f}')\n",
        "    print(f'Test RMSE: {rmse:.4f}')\n",
        "    print(f'Test MAPE: {mape:.2f}%')\n",
        "\n",
        "    return all_predictions, all_targets, test_indices, train_losses, val_losses"
      ],
      "metadata": {
        "id": "0yPz9EINVOEX",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-19T05:10:15.592992Z",
          "iopub.execute_input": "2025-02-19T05:10:15.593404Z",
          "iopub.status.idle": "2025-02-19T05:10:15.608637Z",
          "shell.execute_reply.started": "2025-02-19T05:10:15.593361Z",
          "shell.execute_reply": "2025-02-19T05:10:15.607646Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def duplicate_linear(input):\n",
        "  for i in range(1,3):\n",
        "    np.insert(input,i,input.iloc[:,0],axis=1)\n",
        "  return input"
      ],
      "metadata": {
        "id": "WpxYNSUxrec0",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-19T04:54:33.081718Z",
          "iopub.execute_input": "2025-02-19T04:54:33.082217Z",
          "iopub.status.idle": "2025-02-19T04:54:33.086790Z",
          "shell.execute_reply.started": "2025-02-19T04:54:33.082165Z",
          "shell.execute_reply": "2025-02-19T04:54:33.085781Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Prepare data\n",
        "    input_features, feature_scaler, time_scaler, close_scaler, scaled_close = create_input('AMZN')\n",
        "    pd.DataFrame(input_features).head()\n",
        "\n",
        "    # Create sequences\n",
        "    x, y = create_sequences(input_features, scaled_close)\n",
        "    x_tensor, y_tensor = torch.tensor(x), torch.tensor(y)\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        x_tensor, y_tensor, test_size=0.2, shuffle=False)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    # Initialize model (adjust parameters based on new dimensions)\n",
        "    input_dim = input_features.shape[1]  # (16*2 + 1 time features + 7 original)\n",
        "    d_model = 64\n",
        "    nhead = 4\n",
        "    num_encoder_layers = 2\n",
        "    num_decoder_layers = 2\n",
        "    seq_length = 100\n",
        "\n",
        "    model = StockTransformer(\n",
        "        input_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, seq_length)\n",
        "\n",
        "    # Train and evaluate\n",
        "    predictions, targets, test_indices, train_losses, val_losses = train_and_evaluate(\n",
        "        model, train_loader, test_loader, close_scaler, num_epochs=100)\n"
      ],
      "metadata": {
        "id": "q9YNZZpAVOEj",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-19T05:14:18.785523Z",
          "iopub.execute_input": "2025-02-19T05:14:18.785950Z",
          "iopub.status.idle": "2025-02-19T05:14:18.839699Z",
          "shell.execute_reply.started": "2025-02-19T05:14:18.785916Z",
          "shell.execute_reply": "2025-02-19T05:14:18.838370Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# model architecture\n",
        "model"
      ],
      "metadata": {
        "id": "jntjasd-1sXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Visualization\n",
        "test_dates = tesla.index[-len(predictions):]\n",
        "\n",
        "  # Plot training curves\n",
        "plt.figure(figsize=(6, 3))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot predictions vs actual\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(test_dates, targets, label='Actual')\n",
        "plt.plot(test_dates, predictions, label='Predicted')\n",
        "plt.title('Predicted vs Actual Closing Prices')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zBOF1pTgVOEk",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T17:42:27.334794Z",
          "iopub.execute_input": "2025-02-18T17:42:27.335086Z",
          "iopub.status.idle": "2025-02-18T17:42:27.340481Z",
          "shell.execute_reply.started": "2025-02-18T17:42:27.335063Z",
          "shell.execute_reply": "2025-02-18T17:42:27.339611Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sjza_oltD_Sd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}